{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec50594c",
   "metadata": {},
   "source": [
    "## From CSV to Mysql to Cassandra\n",
    "\n",
    "in this Notebook I'll be using Pyspark to: \n",
    "\n",
    "1 - read from the tweets csv file into a pyspark dataframe\n",
    "\n",
    "2 - saving the pyspark dataframe into a Mysql table (raw data before map reduce)\n",
    "\n",
    "3 - using pyspark to read from mysql table\n",
    "\n",
    "4 - apply reduce and data transformation to the dataframe\n",
    "\n",
    "5 - apply data cleaning,data engineering and sentiment analysis \n",
    "\n",
    "6 - using pyspark for Saving resulting dataframe (post map-reduce) into Cassandra\n",
    "\n",
    "7 - reading from cassandra and create a csv as output for continuing with time serie analysis on another notebook\n",
    "\n",
    "\n",
    "#### connectors for Mysql and Cassandra\n",
    "pyspark --jars mysql-connector-j-8.1.0.jar --packages com.datastax.spark:spark-cassandra-connector_2.12:3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37297eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"MyApp\") \\\n",
    "  .config(\"spark.jars\",  \"mysql-connector-j-8.1.0.jar\") \\\n",
    "  .master(\"local\")\\\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4e6dc",
   "metadata": {},
   "source": [
    "Data can be uploaded normally from csv to mysql and then read it with pyspark\n",
    "\n",
    "in this case I'm uploading csv to mysql using pyspark and then reading it from mysql table to keep all in one notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "13da2fb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 979:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pyspark read from csv\n",
    "data = spark.read.csv(\"/user1/ProjectTweets.csv\", inferSchema=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57852a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pyspark write into Mysql table\n",
    "# sql database is called Tweets and table is called Tweets, schema is already present in mysql (done through CLI)\n",
    "data.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/Tweets\") \\\n",
    "  .option(\"dbtable\", \"Tweets\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"password\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d2aaadbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 982:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pyspark read from Mysql table we just inserted \n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/Tweets\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"Tweets\") \\\n",
    "    .option(\"user\", \"root\").option(\"password\", \"password\").load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300284d6",
   "metadata": {},
   "source": [
    "#### Data engineering using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88646af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 985:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000 ... That's a lot of Data!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if df.count() > 1000000:\n",
    "    print(f\"{data.count()} ... That's a lot of Data!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dd2f69a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c3='NO_QUERY')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique values for _c3\n",
    "df.select('_c3').distinct().collect()\n",
    "#field _c3 only has 1 value, dropping field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "97e7ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df._c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3f76e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 989:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       _c1|count|\n",
      "+----------+-----+\n",
      "|1468544973|    2|\n",
      "|1690908358|    2|\n",
      "|1834777946|    2|\n",
      "|1882160717|    2|\n",
      "|1965601765|    2|\n",
      "|1982434182|    2|\n",
      "|2002309001|    2|\n",
      "|2190980212|    2|\n",
      "|1685304801|    2|\n",
      "|1686371908|    2|\n",
      "|1957194329|    2|\n",
      "|1969964899|    2|\n",
      "|1974268607|    2|\n",
      "|2056807406|    2|\n",
      "|2063670799|    2|\n",
      "|1556266702|    2|\n",
      "|1752414405|    2|\n",
      "|1824843992|    2|\n",
      "|1881996107|    2|\n",
      "|1983726537|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#checking for duplicates\n",
    "df.groupby(\"_c1\").count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2f796bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 994:=============================================>       (171 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duplicates = df.groupby(\"_c1\").count().where(\"count > 1\").drop(\"count\")\n",
    "print(f\"Number of duplicates: {duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8761c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 996:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "|_c0    |_c1       |_c2                         |_c4    |_c5                                                                                          |\n",
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "|252393 |1983726537|Sun May 31 13:42:57 PDT 2009|iargent|Should have gone on a bike ride today but never quite happened  Still enjoyed the sun though |\n",
      "|1190503|1983726537|Sun May 31 13:42:57 PDT 2009|iargent|Should have gone on a bike ride today but never quite happened  Still enjoyed the sun though |\n",
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#show 1 duplicate example\n",
    "df[df[\"_c1\"] == 1983726537].show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b2bf3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates(['_c1'])\n",
    "df.count() #checking how many values after dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "826e2448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca41ad9",
   "metadata": {},
   "source": [
    "Dealing with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d22ecfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mon Apr 06 22:32:38 PDT 2009'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example: \n",
    "df.first()[\"_c2\"] #PDT stands for Pacific time zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2cd9d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1002:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check if all time stamps are in PDT\n",
    "#if all strings have PDT in the timestamp this list should return empty\n",
    "[x for x in df.rdd.toLocalIterator() if \"PDT\" not in x['_c2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3071ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1402:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+-------------------+\n",
      "|       _c1|          _c4|                 _c5|          Timestamp|\n",
      "+----------+-------------+--------------------+-------------------+\n",
      "|1467860144|     Jana1976|@JonathanRKnight ...|2009-04-07 06:32:38|\n",
      "|1467862225|        hdm42|@vjl also, your w...|2009-04-07 06:33:11|\n",
      "|1467889791|jennhelvering|Just called Hills...|2009-04-07 06:40:33|\n",
      "|1467898027|   twitrbug81|@JonathanRKnight ...|2009-04-07 06:42:49|\n",
      "|1467904302|bsbnumber1fan|@nick_carter Aww ...|2009-04-07 06:44:34|\n",
      "+----------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1403:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#all timestamps are PDT\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\") #Had to set as Legacy cause of error \n",
    "#spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"CORRECTED\") #for return to standard timeparser policy\n",
    "\n",
    "Time_Format = \"E MMM d HH:mm:ss z yyyy\"\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(df[\"_c2\"], Time_Format))\n",
    "df = df.drop(df._c2)\n",
    "df = df.drop(df._c0)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "86952e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hashtags\n",
    "from pyspark.sql import functions as f\n",
    "df = df.withColumn(\"Text\",f.regexp_replace(\"_c5\",\"#([^\\s]+)\\s\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5909dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mentions\n",
    "df = df.withColumn(\"Text\",f.regexp_replace(\"Text\",\"@([^\\s]+)\\s\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "76ad930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless\n",
    "df = df.drop(df._c5)\n",
    "df = df.drop(df._c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52d499c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming\n",
    "df = df.withColumnRenamed(\"_c1\",\"id\")\n",
    "df = df.withColumnRenamed(\"Timestamp\",\"timestamp\")\n",
    "df = df.withColumnRenamed(\"Text\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fe24bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5bf5609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Sentiment extraction\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def VAder(x):\n",
    "    return sia.polarity_scores(x)[\"compound\"]\n",
    "\n",
    "# Register the VADER function as a UDF (User-Defined Function)\n",
    "vader_udf = udf(VAder, DoubleType())\n",
    "\n",
    "# Add the 'sentiment' column to the DataFrame using the UDF\n",
    "df = df.withColumn(\"sentiment\", vader_udf(df[\"text\"]))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7943ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, mean\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Group by day and calculate the mean score\n",
    "result_df = df.groupBy(to_date(\"timestamp\").alias(\"Day\")).agg(mean(\"sentiment\").alias(\"score\"))\n",
    "result_df = result_df.sort(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a72c9ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1405:===================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  max(Day)|\n",
      "+----------+\n",
      "|2009-06-25|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1406:======================================>             (147 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.createOrReplaceTempView(\"aggregate\")\n",
    "res = spark.sql(\"SELECT min(Day) from aggregate\")\n",
    "res = spark.sql(\"SELECT max(Day) from aggregate\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7187b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with all dates within min and max\n",
    "import pandas as pd\n",
    "\n",
    "#creating a new Df with same start and end dates\n",
    "dates = []\n",
    "for date in pd.date_range(start=\"2009-04-07\",end=\"2009-06-25\"):\n",
    "    dates.append(date.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "date_df = spark.createDataFrame(dates,\"string\").toDF(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e20f5f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: date (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ec413986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2676db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join to have one dataset with nulls also\n",
    "output = date_df.join(result_df,on='Day',how='left').sort(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "be0b2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1411:==================================================> (196 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|       Day|              score|\n",
      "+----------+-------------------+\n",
      "|2009-04-07|0.15867049362980187|\n",
      "|2009-04-08|               null|\n",
      "|2009-04-09|               null|\n",
      "|2009-04-10|               null|\n",
      "|2009-04-11|               null|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "baea11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching name columns in cassandra\n",
    "output = output.withColumnRenamed(\"Day\",\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0de488cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#session for Cassandra\n",
    "spark = SparkSession.builder\\\n",
    "  .appName(\"MyApp\") \\\n",
    "  .master(\"local[*]\")\\\n",
    "  .getOrCreate()\n",
    "\n",
    "#write into Cassandra\n",
    "#Cassandra keyspace and table is already created (done through CLI)\n",
    "output.write\\\n",
    "  .format('org.apache.spark.sql.cassandra')\\\n",
    "  .mode('append')\\\n",
    "  .options(table='tweets_final',keyspace='tweets_final')\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8cdda4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                day|               score|\n",
      "+-------------------+--------------------+\n",
      "|2009-04-07 00:00:00|0.158670493629801870|\n",
      "|2009-04-08 00:00:00|                null|\n",
      "+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read from cassandra\n",
    "last = spark.read\\\n",
    "  .format('org.apache.spark.sql.cassandra')\\\n",
    "  .options(table='tweets_final',keyspace='tweets_final')\\\n",
    "  .load()\n",
    "last = last.sort(\"Day\")\n",
    "last.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d71a0933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=datetime.datetime(2009, 6, 21, 0, 0), score=Decimal('-0.030768202243756308')),\n",
       " Row(day=datetime.datetime(2009, 6, 22, 0, 0), score=Decimal('-0.052183901689708160')),\n",
       " Row(day=datetime.datetime(2009, 6, 23, 0, 0), score=Decimal('-0.058783732858222630')),\n",
       " Row(day=datetime.datetime(2009, 6, 24, 0, 0), score=Decimal('-0.058545564212113634')),\n",
       " Row(day=datetime.datetime(2009, 6, 25, 0, 0), score=Decimal('-0.051633178163351674'))]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6c46cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#result csv file is then moved from virtual machine to my personal machine for modelling part\n",
    "last.write.csv(\"Downloads/final_output.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
