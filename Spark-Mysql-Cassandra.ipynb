{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec50594c",
   "metadata": {},
   "source": [
    "## From CSV to Mysql to Cassandra\n",
    "\n",
    "in this Notebook I'll be using Pyspark to: \n",
    "\n",
    "1 - read from the tweets csv file into a pyspark dataframe\n",
    "\n",
    "2 - saving the pyspark dataframe into a Mysql table (raw data before map reduce)\n",
    "\n",
    "3 - using pyspark to read from mysql table\n",
    "\n",
    "4 - apply reduce and data transformation to the dataframe\n",
    "\n",
    "5 - apply data cleaning,data engineering and sentiment analysis \n",
    "\n",
    "6 - using pyspark for Saving resulting dataframe (post map-reduce) into Cassandra\n",
    "\n",
    "7 - reading from cassandra and create a csv as output for continuing with time serie analysis on another notebook\n",
    "\n",
    "\n",
    "#### connectors for Mysql and Cassandra\n",
    "pyspark --jars mysql-connector-j-8.1.0.jar --packages com.datastax.spark:spark-cassandra-connector_2.12:3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37297eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"MyApp\") \\\n",
    "  .config(\"spark.jars\",  \"mysql-connector-j-8.1.0.jar\") \\\n",
    "  .master(\"local\")\\\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4e6dc",
   "metadata": {},
   "source": [
    "Data can be uploaded normally from csv to mysql and then read it with pyspark\n",
    "\n",
    "in this case I'm uploading csv to mysql using pyspark and then reading it from mysql table to keep all in one notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13da2fb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark read from csv\n",
    "data = spark.read.csv(\"/user1/ProjectTweets.csv\", inferSchema=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57852a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pyspark write into Mysql table\n",
    "# sql database is called Tweets and table is called Tweets, schema is already present in mysql (done through CLI)\n",
    "data.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/Tweets\") \\\n",
    "  .option(\"dbtable\", \"Tweets\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"password\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aaadbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pyspark read from Mysql table we just inserted \n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/Tweets\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"Tweets\") \\\n",
    "    .option(\"user\", \"root\").option(\"password\", \"password\").load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300284d6",
   "metadata": {},
   "source": [
    "#### Data engineering using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88646af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000 ... That's a lot of Data!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if df.count() > 1000000:\n",
    "    print(f\"{data.count()} ... That's a lot of Data!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2f69a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c3='NO_QUERY')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique values for _c3\n",
    "df.select('_c3').distinct().collect()\n",
    "#field _c3 only has 1 value, dropping field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e7ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df._c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f76e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       _c1|count|\n",
      "+----------+-----+\n",
      "|1468544973|    2|\n",
      "|1690908358|    2|\n",
      "|1834777946|    2|\n",
      "|1882160717|    2|\n",
      "|1965601765|    2|\n",
      "|1982434182|    2|\n",
      "|2002309001|    2|\n",
      "|2190980212|    2|\n",
      "|1685304801|    2|\n",
      "|1686371908|    2|\n",
      "|1957194329|    2|\n",
      "|1969964899|    2|\n",
      "|1974268607|    2|\n",
      "|2056807406|    2|\n",
      "|2063670799|    2|\n",
      "|1556266702|    2|\n",
      "|1752414405|    2|\n",
      "|1824843992|    2|\n",
      "|1881996107|    2|\n",
      "|1983726537|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for duplicates\n",
    "df.groupby(\"_c1\").count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f796bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================================================>    (184 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:=====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duplicates = df.groupby(\"_c1\").count().where(\"count > 1\").drop(\"count\")\n",
    "print(f\"Number of duplicates: {duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8761c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "|_c0    |_c1       |_c2                         |_c4    |_c5                                                                                          |\n",
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "|252393 |1983726537|Sun May 31 13:42:57 PDT 2009|iargent|Should have gone on a bike ride today but never quite happened  Still enjoyed the sun though |\n",
      "|1190503|1983726537|Sun May 31 13:42:57 PDT 2009|iargent|Should have gone on a bike ride today but never quite happened  Still enjoyed the sun though |\n",
      "+-------+----------+----------------------------+-------+---------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#show 1 duplicate example\n",
    "df[df[\"_c1\"] == 1983726537].show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2bf3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates(['_c1'])\n",
    "df.count() #checking how many values after dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826e2448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca41ad9",
   "metadata": {},
   "source": [
    "Dealing with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22ecfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mon Apr 06 22:32:38 PDT 2009'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example: \n",
    "df.first()[\"_c2\"] #PDT stands for Pacific time zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd9d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check if all time stamps are in PDT\n",
    "#if all strings have PDT in the timestamp this list should return empty\n",
    "[x for x in df.rdd.toLocalIterator() if \"PDT\" not in x['_c2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3071ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 424:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+-------------------+\n",
      "|       _c1|          _c4|                 _c5|          Timestamp|\n",
      "+----------+-------------+--------------------+-------------------+\n",
      "|1467860144|     Jana1976|@JonathanRKnight ...|2009-04-07 06:32:38|\n",
      "|1467862225|        hdm42|@vjl also, your w...|2009-04-07 06:33:11|\n",
      "|1467889791|jennhelvering|Just called Hills...|2009-04-07 06:40:33|\n",
      "|1467898027|   twitrbug81|@JonathanRKnight ...|2009-04-07 06:42:49|\n",
      "|1467904302|bsbnumber1fan|@nick_carter Aww ...|2009-04-07 06:44:34|\n",
      "+----------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#all timestamps are PDT\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\") #Had to set as Legacy cause of error \n",
    "#spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"CORRECTED\") #for return to standard timeparser policy\n",
    "\n",
    "Time_Format = \"E MMM d HH:mm:ss z yyyy\"\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(df[\"_c2\"], Time_Format))\n",
    "df = df.drop(df._c2)\n",
    "df = df.drop(df._c0)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86952e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#cleaning and sentiment analysis\n",
    "import nltk\n",
    "import re\n",
    "from pyspark.sql import functions as f\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')  # Download the necessary data for NLTK\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.types import StringType\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def stem(text):\n",
    "    #remove tags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    ##stop words #stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_text = ' '.join([stemmer.stem(token) for token in tokens if token not in stop_words ])\n",
    "    \n",
    "    return stemmed_text\n",
    "\n",
    "stem_udf = udf(stem, StringType())\n",
    "df = df.withColumn(\"text\", stem_udf(df[\"_c5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cca946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 427:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+-------------------+--------------------+\n",
      "|       _c1|           _c4|                 _c5|          Timestamp|                text|\n",
      "+----------+--------------+--------------------+-------------------+--------------------+\n",
      "|1467860144|      Jana1976|@JonathanRKnight ...|2009-04-07 06:32:38|i hate limit lett...|\n",
      "|1467862225|         hdm42|@vjl also, your w...|2009-04-07 06:33:11|also websit seem ...|\n",
      "|1467889791| jennhelvering|Just called Hills...|2009-04-07 06:40:33|just call hillson...|\n",
      "|1467898027|    twitrbug81|@JonathanRKnight ...|2009-04-07 06:42:49|         thought mac|\n",
      "|1467904302| bsbnumber1fan|@nick_carter Aww ...|2009-04-07 06:44:34|aww nick i like h...|\n",
      "|1467928749|      calliott|is tireddddddd. w...|2009-04-07 06:51:26|tireddddddd want ...|\n",
      "|1467946810|TheDarrenxshow|@ilovepie mines t...|2009-04-07 06:56:37|mine im find well...|\n",
      "|1467968979|     atothebed|@clarianne APRIL ...|2009-04-07 07:02:45|april 9th isnt co...|\n",
      "|1467987384|   vardenrhode|Just published a ...|2009-04-07 07:08:02|just publish new ...|\n",
      "|1468005581|     Yahtzee27|@littrellfans Its...|2009-04-07 07:13:16|it good just figu...|\n",
      "|1468010346|  Kelsey_Leigh|Why does school t...|2009-04-07 07:14:43|whi school take l...|\n",
      "|1468038360|   serendipify|@deon - &quot;sou...|2009-04-07 07:23:22|quotsourc shine2q...|\n",
      "|1468070706|        gblock|@kellyleahy Not t...|2009-04-07 07:33:20|       not mani file|\n",
      "|1468071555|         Feuza|@julesbianchi OMG...|2009-04-07 07:33:35|omg particip fusi...|\n",
      "|1468071701|     Mojo4Melo|@efng   &quot;Now...|2009-04-07 07:33:38|quotnow wheelbarr...|\n",
      "|1468088102|  room2breathe|@veronica78 I saw...|2009-04-07 07:38:52| i saw got chang hot|\n",
      "|1468108670|natalieantipas|@ryleebeth ye im ...|2009-04-07 07:45:38|ye im sadbut weir...|\n",
      "|1468115212|  MyAppleStuff|@Weebeedee run wa...|2009-04-07 07:47:49|run great thank i...|\n",
      "|1468132343|  kimmiecubaby|             is cold|2009-04-07 07:53:34|                cold|\n",
      "|1468132370|      AeroMint|@Tyrese4ReaL Than...|2009-04-07 07:53:35|           thank man|\n",
      "+----------+--------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ad930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless\n",
    "df = df.drop(df._c5)\n",
    "df = df.drop(df._c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52d499c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming\n",
    "df = df.withColumnRenamed(\"_c1\",\"id\")\n",
    "df = df.withColumnRenamed(\"Timestamp\",\"timestamp\")\n",
    "df = df.withColumnRenamed(\"Text\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe24bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bf5609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- sentiment: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sentiment extraction\n",
    "\n",
    "def VAder(x):\n",
    "    return sia.polarity_scores(x)[\"compound\"]\n",
    "\n",
    "# Register the VADER function as a UDF (User-Defined Function)\n",
    "vader_udf = udf(VAder, DoubleType())\n",
    "\n",
    "# Add the 'sentiment' column to the DataFrame using the UDF\n",
    "df = df.withColumn(\"sentiment\", vader_udf(df[\"text\"]))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7943ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, mean\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Group by day and calculate the mean score\n",
    "result_df = df.groupBy(to_date(\"timestamp\").alias(\"Day\")).agg(mean(\"sentiment\").alias(\"score\"))\n",
    "result_df = result_df.sort(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a72c9ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 430:======================================>              (144 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  max(Day)|\n",
      "+----------+\n",
      "|2009-06-25|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 430:===============================================>     (181 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.createOrReplaceTempView(\"aggregate\")\n",
    "res = spark.sql(\"SELECT min(Day) from aggregate\")\n",
    "res = spark.sql(\"SELECT max(Day) from aggregate\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7187b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with all dates within min and max\n",
    "import pandas as pd\n",
    "\n",
    "#creating a new Df with same start and end dates\n",
    "dates = []\n",
    "for date in pd.date_range(start=\"2009-04-07\",end=\"2009-06-25\"):\n",
    "    dates.append(date.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "date_df = spark.createDataFrame(dates,\"string\").toDF(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e20f5f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: date (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec413986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2676db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join to have one dataset with nulls also\n",
    "output = date_df.join(result_df,on='Day',how='left').sort(\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be0b2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|       Day|              score|\n",
      "+----------+-------------------+\n",
      "|2009-04-07|0.13217767281887322|\n",
      "|2009-04-08|               null|\n",
      "|2009-04-09|               null|\n",
      "|2009-04-10|               null|\n",
      "|2009-04-11|               null|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baea11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching name columns in cassandra\n",
    "output = output.withColumnRenamed(\"Day\",\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0de488cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#session for Cassandra\n",
    "spark = SparkSession.builder\\\n",
    "  .appName(\"MyApp\") \\\n",
    "  .master(\"local[*]\")\\\n",
    "  .getOrCreate()\n",
    "\n",
    "#write into Cassandra\n",
    "#Cassandra keyspace and table is already created (done through CLI)\n",
    "output.write\\\n",
    "  .format('org.apache.spark.sql.cassandra')\\\n",
    "  .mode('overwrite')\\\n",
    "  .options(table='tweets_final',keyspace='tweets_final')\\\n",
    "  .option(\"confirm.truncate\", \"true\")\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cdda4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                day|               score|\n",
      "+-------------------+--------------------+\n",
      "|2009-04-07 00:00:00|0.132177672818873220|\n",
      "|2009-04-08 00:00:00|                null|\n",
      "+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read from cassandra\n",
    "last = spark.read\\\n",
    "  .format('org.apache.spark.sql.cassandra')\\\n",
    "  .options(table='tweets_final',keyspace='tweets_final')\\\n",
    "  .load()\n",
    "last = last.sort(\"Day\")\n",
    "last.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c46cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#result csv file is then moved from virtual machine to my personal machine for modelling part\n",
    "last.write.csv(\"Downloads/final_output.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
